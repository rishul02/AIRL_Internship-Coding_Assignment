{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5h7L7qpUF72/AkHNEchXX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN-Sv9Yrpywa"
      },
      "outputs": [],
      "source": [
        "print(\"Setting up system prerequisites...\")\n",
        "\n",
        "# Install system dependencies first\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq build-essential python3-dev libglib2.0-0 libsm6 libxext6 libxrender-dev\n",
        "\n",
        "print(\"✓ System dependencies installed\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Install Core Packages\n",
        "# ============================================================================\n",
        "print(\"Installing core packages...\")\n",
        "print(\"This may take 5-8 minutes on first run.\\n\")\n",
        "\n",
        "# Install PyTorch and torchvision first (if not already installed)\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Install other dependencies\n",
        "!pip install -q opencv-python pillow matplotlib numpy tqdm\n",
        "!pip install -q supervision\n",
        "\n",
        "print(\"✓ Core packages installed\\n\")"
      ],
      "metadata": {
        "id": "ZaKXEejwqPKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Install SAM 2\n",
        "# ============================================================================\n",
        "print(\"Installing SAM 2...\")\n",
        "\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything-2.git\n",
        "\n",
        "print(\"✓ SAM 2 installed\\n\")\n"
      ],
      "metadata": {
        "id": "VK0BjerPqT6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Install GroundingDINO (Fixed Method)\n",
        "# ============================================================================\n",
        "print(\"Installing GroundingDINO...\")\n",
        "print(\"Using alternative installation method...\\n\")\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Method 1: Try direct install with build isolation disabled\n",
        "try:\n",
        "    print(\"Attempting Method 1: Direct install...\")\n",
        "    !pip install -q --no-build-isolation git+https://github.com/IDEA-Research/GroundingDINO.git\n",
        "    print(\"✓ GroundingDINO installed successfully (Method 1)\\n\")\n",
        "    GROUNDING_INSTALLED = True\n",
        "except Exception as e:\n",
        "    print(f\"Method 1 failed, trying Method 2...\\n\")\n",
        "    GROUNDING_INSTALLED = False\n",
        "\n",
        "# Method 2: Clone and install manually\n",
        "if not GROUNDING_INSTALLED:\n",
        "    try:\n",
        "        print(\"Method 2: Manual installation from source...\")\n",
        "\n",
        "        # Clone repository\n",
        "        if not os.path.exists(\"GroundingDINO\"):\n",
        "            !git clone -q https://github.com/IDEA-Research/GroundingDINO.git\n",
        "\n",
        "        os.chdir(\"GroundingDINO\")\n",
        "\n",
        "        # Install dependencies\n",
        "        !pip install -q -r requirements.txt\n",
        "\n",
        "        # Install package\n",
        "        !pip install -q -e .\n",
        "\n",
        "        os.chdir(\"..\")\n",
        "\n",
        "        print(\"✓ GroundingDINO installed successfully (Method 2)\\n\")\n",
        "        GROUNDING_INSTALLED = True\n",
        "    except Exception as e:\n",
        "        print(f\"Method 2 failed: {e}\\n\")\n",
        "        GROUNDING_INSTALLED = False\n",
        "\n",
        "# Method 3: Fallback to pre-built alternative (GLIP or OWLv2)\n",
        "if not GROUNDING_INSTALLED:\n",
        "    print(\"⚠ GroundingDINO installation failed.\")\n",
        "    print(\"Using fallback: OWL-ViT (Hugging Face Transformers)\\n\")\n",
        "    !pip install -q transformers\n",
        "    USE_FALLBACK = True\n",
        "else:\n",
        "    USE_FALLBACK = False\n",
        "\n",
        "print(\"=\"*70)\n",
        "if not USE_FALLBACK:\n",
        "    print(\"✓ All packages installed successfully!\")\n",
        "else:\n",
        "    print(\"✓ Packages installed with fallback detection model\")\n",
        "print(\"=\"*70 + \"\\n\")\n"
      ],
      "metadata": {
        "id": "nJh1oMTBqaKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Imports\n",
        "# ============================================================================\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple, Optional\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print()\n",
        "\n",
        "# Conditional imports based on installation success\n",
        "if not USE_FALLBACK:\n",
        "    try:\n",
        "        from groundingdino.util.inference import load_model as load_grounding_model\n",
        "        from groundingdino.util.inference import predict as grounding_predict\n",
        "        from groundingdino.util.inference import annotate\n",
        "        print(\"✓ GroundingDINO imported successfully\")\n",
        "    except ImportError as e:\n",
        "        print(f\"⚠ GroundingDINO import failed: {e}\")\n",
        "        print(\"Switching to fallback model...\")\n",
        "        USE_FALLBACK = True\n",
        "\n",
        "if USE_FALLBACK:\n",
        "    from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
        "    print(\"✓ Using OWL-ViT as fallback detector\")\n",
        "\n",
        "# SAM 2 imports\n",
        "from sam2.build_sam import build_sam2\n",
        "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
        "from sam2.build_sam import build_sam2_video_predictor\n",
        "\n",
        "print(\"✓ SAM 2 imported successfully\\n\")"
      ],
      "metadata": {
        "id": "__1PlSWAqizZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# CELL 6: Text-Driven Segmentation Pipeline\n",
        "# ==============================================================\n",
        "\n",
        "class TextDrivenSegmentation:\n",
        "    def __init__(self, sam2_predictor, device, use_fallback=False):\n",
        "        self.sam2_predictor = sam2_predictor\n",
        "        self.device = device\n",
        "        self.use_fallback = use_fallback\n",
        "        if not use_fallback:\n",
        "            self.grounding_model = grounding_model\n",
        "        else:\n",
        "            self.owl_processor = owl_processor\n",
        "            self.owl_model = owl_model\n",
        "\n",
        "    def detect_objects(self, image: np.ndarray, text_prompt: str,\n",
        "                       box_threshold: float = 0.25, text_threshold: float = 0.25):\n",
        "        if not self.use_fallback:\n",
        "            from torchvision.ops import box_convert\n",
        "            image_pil = Image.fromarray(image)\n",
        "            boxes, logits, phrases = grounding_predict(\n",
        "                model=self.grounding_model,\n",
        "                image=image_pil,\n",
        "                caption=text_prompt,\n",
        "                box_threshold=box_threshold,\n",
        "                text_threshold=text_threshold,\n",
        "                device=self.device\n",
        "            )\n",
        "            h, w = image.shape[:2]\n",
        "            boxes = boxes * torch.tensor([w, h, w, h], device=boxes.device)\n",
        "            boxes = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\")\n",
        "            return boxes.cpu().numpy(), logits.cpu().numpy(), phrases\n",
        "        else:\n",
        "            image_pil = Image.fromarray(image)\n",
        "            texts = [t.strip() for t in text_prompt.replace(\" . \", \",\").split(\",\")]\n",
        "            inputs = self.owl_processor(text=texts, images=image_pil, return_tensors=\"pt\")\n",
        "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "            with torch.no_grad():\n",
        "                outputs = self.owl_model(**inputs)\n",
        "            target_sizes = torch.tensor([image.shape[:2]]).to(self.device)\n",
        "            results = self.owl_processor.post_process_object_detection(outputs=outputs, target_sizes=target_sizes, threshold=box_threshold)[0]\n",
        "            boxes = results[\"boxes\"].cpu().numpy()\n",
        "            labels = [texts[l] for l in results[\"labels\"].cpu().numpy()]\n",
        "            return boxes, results[\"scores\"].cpu().numpy(), labels\n",
        "\n",
        "    def segment_from_boxes(self, image: np.ndarray, boxes: np.ndarray):\n",
        "        if len(boxes) == 0: return np.array([])\n",
        "        self.sam2_predictor.set_image(image)\n",
        "        masks_list = []\n",
        "        for box in boxes:\n",
        "            masks, _, _ = self.sam2_predictor.predict(None, None, box[None, :], multimask_output=False)\n",
        "            masks_list.append(masks[0])\n",
        "        return np.array(masks_list)\n",
        "\n",
        "    def segment_from_text(self, image: np.ndarray, text_prompt: str, box_threshold=0.25, text_threshold=0.25):\n",
        "        boxes, _, labels = self.detect_objects(image, text_prompt, box_threshold, text_threshold)\n",
        "        if len(boxes) == 0: return np.array([]), boxes, labels\n",
        "        masks = self.segment_from_boxes(image, boxes)\n",
        "        return masks, boxes, labels\n",
        "\n",
        "pipeline = TextDrivenSegmentation(sam2_predictor, device, USE_FALLBACK)\n"
      ],
      "metadata": {
        "id": "eFPnHA6RqorL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Load Models\n",
        "# ============================================================================\n",
        "print(\"Loading models into memory...\")\n",
        "\n",
        "# First, let's check what SAM 2 configs are available\n",
        "print(\"Checking available SAM 2 configs...\")\n",
        "try:\n",
        "    import sam2\n",
        "    sam2_path = os.path.dirname(sam2.__file__)\n",
        "    configs_path = os.path.join(sam2_path, \"configs\")\n",
        "    print(f\"SAM 2 package path: {sam2_path}\")\n",
        "\n",
        "    # List available configs\n",
        "    if os.path.exists(configs_path):\n",
        "        print(f\"Configs directory: {configs_path}\")\n",
        "        for root, dirs, files in os.walk(configs_path):\n",
        "            for file in files:\n",
        "                if file.endswith('.yaml'):\n",
        "                    rel_path = os.path.relpath(os.path.join(root, file), configs_path)\n",
        "                    print(f\"  Found config: {rel_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not list configs: {e}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Load detection model\n",
        "if not USE_FALLBACK:\n",
        "    print(\"Loading GroundingDINO...\")\n",
        "    try:\n",
        "        # Try to find the config in the installed package\n",
        "        if os.path.exists(\"GroundingDINO\"):\n",
        "            GROUNDING_CONFIG = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
        "        else:\n",
        "            # If GroundingDINO was installed via pip, try to find config in site-packages\n",
        "            import groundingdino\n",
        "            package_path = os.path.dirname(groundingdino.__file__)\n",
        "            GROUNDING_CONFIG = os.path.join(package_path, \"config\", \"GroundingDINO_SwinT_OGC.py\")\n",
        "\n",
        "        grounding_model = load_grounding_model(GROUNDING_CONFIG, GROUNDING_CHECKPOINT, device=device)\n",
        "        print(\"✓ GroundingDINO loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Failed to load GroundingDINO: {e}\")\n",
        "        print(\"Switching to fallback...\")\n",
        "        USE_FALLBACK = True\n",
        "\n",
        "if USE_FALLBACK:\n",
        "    print(\"Loading OWL-ViT...\")\n",
        "    owl_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "    owl_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\").to(device)\n",
        "    owl_model.eval()\n",
        "    print(\"✓ OWL-ViT loaded\")\n",
        "\n",
        "# Load SAM 2 with correct config path\n",
        "print(\"Loading SAM 2 Image Predictor...\")\n",
        "\n",
        "# The configs are in subdirectories, and we need to use the correct format\n",
        "# Based on the directory listing, we see: sam2.1/sam2.1_hiera_l.yaml\n",
        "# But we need to use hydra config path format\n",
        "\n",
        "from hydra import compose, initialize_config_dir\n",
        "from hydra.core.global_hydra import GlobalHydra\n",
        "\n",
        "# Try using hydra to load the config properly\n",
        "sam2_model = None\n",
        "\n",
        "try:\n",
        "    # Method 1: Use hydra compose with the config path\n",
        "    print(\"  Trying Method 1: Hydra compose...\")\n",
        "\n",
        "    # Get the config directory\n",
        "    import sam2\n",
        "    sam2_path = os.path.dirname(sam2.__file__)\n",
        "    config_dir = os.path.join(sam2_path, \"configs\", \"sam2.1\")\n",
        "\n",
        "    # Initialize hydra with the config directory\n",
        "    GlobalHydra.instance().clear()\n",
        "    initialize_config_dir(config_dir=config_dir, version_base=None)\n",
        "    cfg = compose(config_name=\"sam2.1_hiera_l\")\n",
        "\n",
        "    # Now build the model using the hydra config\n",
        "    from sam2.modeling.sam2_base import SAM2Base\n",
        "    sam2_model = SAM2Base(\n",
        "        image_encoder=cfg.image_encoder,\n",
        "        memory_attention=cfg.memory_attention,\n",
        "        memory_encoder=cfg.memory_encoder,\n",
        "        num_maskmem=cfg.num_maskmem,\n",
        "        image_size=cfg.image_size,\n",
        "        backbone_stride=cfg.backbone_stride,\n",
        "        sigmoid_scale_for_mem_enc=cfg.sigmoid_scale_for_mem_enc,\n",
        "        sigmoid_bias_for_mem_enc=cfg.sigmoid_bias_for_mem_enc,\n",
        "        binarize_mask_from_pts_for_mem_enc=cfg.binarize_mask_from_pts_for_mem_enc,\n",
        "        use_mask_input_as_output_without_sam=cfg.use_mask_input_as_output_without_sam,\n",
        "        max_cond_frames_in_attn=cfg.max_cond_frames_in_attn,\n",
        "        directly_add_no_mem_embed=cfg.directly_add_no_mem_embed,\n",
        "        use_high_res_features_in_sam=cfg.use_high_res_features_in_sam,\n",
        "        multimask_output_in_sam=cfg.multimask_output_in_sam,\n",
        "        multimask_min_pt_num=cfg.multimask_min_pt_num,\n",
        "        multimask_max_pt_num=cfg.multimask_max_pt_num,\n",
        "        multimask_output_for_tracking=cfg.multimask_output_for_tracking,\n",
        "        use_multimask_token_for_obj_ptr=cfg.use_multimask_token_for_obj_ptr,\n",
        "        compile_image_encoder=cfg.compile_image_encoder,\n",
        "        iou_prediction_use_sigmoid=cfg.iou_prediction_use_sigmoid,\n",
        "        memory_temporal_stride_for_eval=cfg.memory_temporal_stride_for_eval,\n",
        "        non_overlap_masks_for_mem_enc=cfg.non_overlap_masks_for_mem_enc,\n",
        "        use_obj_ptrs_in_encoder=cfg.use_obj_ptrs_in_encoder,\n",
        "        max_obj_ptrs_in_encoder=cfg.max_obj_ptrs_in_encoder,\n",
        "        add_all_frames_to_correct_as_cond=cfg.add_all_frames_to_correct_as_cond,\n",
        "        pred_obj_scores=cfg.pred_obj_scores,\n",
        "        pred_obj_scores_mlp=cfg.pred_obj_scores_mlp,\n",
        "        fixed_no_obj_ptr=cfg.fixed_no_obj_ptr,\n",
        "        soft_no_obj_ptr=cfg.soft_no_obj_ptr,\n",
        "        use_mlp_for_obj_ptr_proj=cfg.use_mlp_for_obj_ptr_proj,\n",
        "        sam_mask_decoder_extra_args=cfg.sam_mask_decoder_extra_args,\n",
        "    )\n",
        "\n",
        "    # Load checkpoint\n",
        "    state_dict = torch.load(SAM2_CHECKPOINT, map_location=device)\n",
        "    sam2_model.load_state_dict(state_dict, strict=False)\n",
        "    sam2_model = sam2_model.to(device)\n",
        "    sam2_model.eval()\n",
        "\n",
        "    SAM2_CONFIG = \"sam2.1/sam2.1_hiera_l\"\n",
        "    print(\"✓ SAM 2 loaded using Hydra compose\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  Method 1 failed: {str(e)[:200]}\")\n",
        "\n",
        "    # Method 2: Use the checkpoint directly with the correct config path\n",
        "    try:\n",
        "        print(\"  Trying Method 2: Direct config path...\")\n",
        "        # Try using relative path from sam2 package\n",
        "        SAM2_CONFIG = \"sam2.1/sam2.1_hiera_l.yaml\"\n",
        "\n",
        "        # Need to manually specify config dir\n",
        "        import sam2\n",
        "        from hydra import compose, initialize\n",
        "        from hydra.core.global_hydra import GlobalHydra\n",
        "\n",
        "        GlobalHydra.instance().clear()\n",
        "\n",
        "        # Initialize with sam2 configs directory\n",
        "        with initialize(version_base=None, config_path=\"../../../sam2/configs\"):\n",
        "            cfg = compose(config_name=\"sam2.1/sam2.1_hiera_l\")\n",
        "\n",
        "        from sam2.build_sam import _build_sam2\n",
        "        sam2_model = _build_sam2(cfg, SAM2_CHECKPOINT, device=device)\n",
        "        print(\"✓ SAM 2 loaded using direct config path\")\n",
        "\n",
        "    except Exception as e2:\n",
        "        print(f\"  Method 2 failed: {str(e2)[:200]}\")\n",
        "\n",
        "        # Method 3: Use programmatic config\n",
        "        print(\"  Trying Method 3: Programmatic loading...\")\n",
        "        try:\n",
        "            from sam2.build_sam import build_sam2_from_config\n",
        "\n",
        "            # Build config dict manually for sam2.1_hiera_l\n",
        "            config_dict = {\n",
        "                'image_encoder': {\n",
        "                    '_target_': 'sam2.modeling.backbones.hieradet.Hiera',\n",
        "                    'embed_dim': 144,\n",
        "                    'num_heads': 2,\n",
        "                    'stages': [2, 6, 36, 4],\n",
        "                    'global_att_blocks': [23, 33, 43],\n",
        "                    'window_pos_embed_bkg_spatial_size': [7, 7],\n",
        "                },\n",
        "                'memory_attention': {\n",
        "                    '_target_': 'sam2.modeling.memory_attention.MemoryAttention',\n",
        "                    'd_model': 256,\n",
        "                    'pos_enc_at_input': True,\n",
        "                    'layer': {\n",
        "                        '_target_': 'sam2.modeling.memory_attention.MemoryAttentionLayer',\n",
        "                        'activation': 'relu',\n",
        "                        'dim_feedforward': 2048,\n",
        "                        'd_model': 256,\n",
        "                        'dropout': 0.1,\n",
        "                        'pos_enc_at_attn': False,\n",
        "                        'self_attention': {\n",
        "                            '_target_': 'sam2.modeling.sam.transformer.RoPEAttention',\n",
        "                            'rope_theta': 10000.0,\n",
        "                            'feat_sizes': [32, 32],\n",
        "                            'embedding_dim': 256,\n",
        "                            'num_heads': 1,\n",
        "                            'downsample_rate': 1,\n",
        "                            'dropout': 0.1,\n",
        "                        },\n",
        "                        'd_model': 256,\n",
        "                        'pos_enc_at_cross_attn_keys': True,\n",
        "                        'pos_enc_at_cross_attn_queries': False,\n",
        "                        'cross_attention': {\n",
        "                            '_target_': 'sam2.modeling.sam.transformer.RoPEAttention',\n",
        "                            'rope_theta': 10000.0,\n",
        "                            'feat_sizes': [32, 32],\n",
        "                            'rope_k_repeat': True,\n",
        "                            'embedding_dim': 256,\n",
        "                            'num_heads': 1,\n",
        "                            'downsample_rate': 1,\n",
        "                            'dropout': 0.1,\n",
        "                            'kv_in_dim': 128,\n",
        "                        },\n",
        "                    },\n",
        "                    'num_layers': 4,\n",
        "                },\n",
        "                # Add other necessary config fields\n",
        "                'num_maskmem': 7,\n",
        "                'image_size': 1024,\n",
        "                'backbone_stride': 16,\n",
        "                'use_high_res_features_in_sam': True,\n",
        "                'multimask_output_in_sam': True,\n",
        "            }\n",
        "\n",
        "            # This is getting complex, let's try simpler approach\n",
        "            raise NotImplementedError(\"Config too complex\")\n",
        "\n",
        "        except Exception as e3:\n",
        "            print(f\"  Method 3 failed: {str(e3)[:200]}\")\n",
        "\n",
        "if sam2_model is None:\n",
        "    print(\"\\n⚠ All automatic methods failed. Trying simple workaround...\")\n",
        "    print(\"Using build_sam2 with corrected import...\")\n",
        "\n",
        "    # Final method: patch the config search path\n",
        "    import sam2\n",
        "    from hydra import compose, initialize_config_dir\n",
        "    from hydra.core.global_hydra import GlobalHydra\n",
        "\n",
        "    GlobalHydra.instance().clear()\n",
        "\n",
        "    sam2_config_dir = os.path.join(os.path.dirname(sam2.__file__), \"configs\")\n",
        "\n",
        "    # Use initialize_config_dir which allows us to specify exact directory\n",
        "    from hydra import initialize, compose\n",
        "\n",
        "    with initialize(version_base=None, config_path=\"../../\" + sam2_config_dir):\n",
        "        cfg = compose(config_name=\"sam2.1/sam2.1_hiera_l.yaml\")\n",
        "\n",
        "    sam2_model = build_sam2(cfg, SAM2_CHECKPOINT, device=device)\n",
        "    SAM2_CONFIG = \"sam2.1/sam2.1_hiera_l.yaml\"\n",
        "    print(\"✓ SAM 2 loaded with workaround\")\n",
        "\n",
        "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
        "\n",
        "print(\"Loading SAM 2 Video Predictor...\")\n",
        "sam2_video_predictor = build_sam2_video_predictor(SAM2_CONFIG, SAM2_CHECKPOINT, device=device)\n",
        "print(\"✓ SAM 2 Video Predictor loaded\")\n",
        "\n",
        "print(\"\\n✓ All models loaded!\\n\")\n"
      ],
      "metadata": {
        "id": "uVbN0k-GquWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Load Models\n",
        "# ============================================================================\n",
        "print(\"Loading models into memory...\")\n",
        "\n",
        "# Load detection model\n",
        "if not USE_FALLBACK:\n",
        "    print(\"Loading GroundingDINO...\")\n",
        "    try:\n",
        "        grounding_model = load_grounding_model(GROUNDING_CONFIG, GROUNDING_CHECKPOINT, device=device)\n",
        "        print(\"✓ GroundingDINO loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Failed to load GroundingDINO: {e}\")\n",
        "        print(\"Switching to fallback...\")\n",
        "        USE_FALLBACK = True\n",
        "\n",
        "if USE_FALLBACK:\n",
        "    print(\"Loading OWL-ViT...\")\n",
        "    owl_processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "    owl_model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\").to(device)\n",
        "    owl_model.eval()\n",
        "    print(\"✓ OWL-ViT loaded\")\n",
        "\n",
        "# Load SAM 2\n",
        "print(\"Loading SAM 2 Image Predictor...\")\n",
        "sam2_model = build_sam2(SAM2_CONFIG, SAM2_CHECKPOINT, device=device)\n",
        "sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
        "print(\"✓ SAM 2 Image Predictor loaded\")\n",
        "\n",
        "print(\"Loading SAM 2 Video Predictor...\")\n",
        "sam2_video_predictor = build_sam2_video_predictor(SAM2_CONFIG, SAM2_CHECKPOINT, device=device)\n",
        "print(\"✓ SAM 2 Video Predictor loaded\")\n",
        "\n",
        "print(\"\\n✓ All models loaded!\\n\")"
      ],
      "metadata": {
        "id": "JlHkMe82q4NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Example 1 - Dog Segmentation\n",
        "# ============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"EXAMPLE 1: Segmenting a Dog\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "!wget -q -O sample_dog.jpg https://images.unsplash.com/photo-1587300003388-59208cc962cb?w=800\n",
        "\n",
        "image = cv2.imread(\"sample_dog.jpg\")\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "text_prompt = \"dog\"\n",
        "masks, boxes, labels = pipeline.segment_from_text(image, text_prompt, box_threshold=0.2)\n",
        "\n",
        "visualize_results(image, masks, boxes, labels, text_prompt)"
      ],
      "metadata": {
        "id": "FxqAmE6drOkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 10: Example 2 - Multiple Objects\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXAMPLE 2: Multiple Objects\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "!wget -q -O sample_scene.jpg https://images.unsplash.com/photo-1555041469-a586c61ea9bc?w=800\n",
        "\n",
        "image2 = cv2.imread(\"sample_scene.jpg\")\n",
        "image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Try multiple objects\n",
        "if not USE_FALLBACK:\n",
        "    text_prompt2 = \"sofa . table . lamp\"  # GroundingDINO format\n",
        "else:\n",
        "    text_prompt2 = \"sofa, table, lamp\"  # OWL-ViT format\n",
        "\n",
        "masks2, boxes2, labels2 = pipeline.segment_from_text(image2, text_prompt2, box_threshold=0.15)\n",
        "\n",
        "visualize_results(image2, masks2, boxes2, labels2, text_prompt2)"
      ],
      "metadata": {
        "id": "0IBrzcU2rUra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Interactive Upload\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"INTERACTIVE: Upload Your Own Image\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload an image:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if uploaded:\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    user_image = cv2.imread(filename)\n",
        "    user_image = cv2.cvtColor(user_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    print(\"\\nImage loaded!\")\n",
        "    print(\"\\nEnter text prompt:\")\n",
        "    if not USE_FALLBACK:\n",
        "        print(\"  Format: 'object1 . object2 . object3'\")\n",
        "        print(\"  Example: 'cat . dog . person'\")\n",
        "    else:\n",
        "        print(\"  Format: 'object1, object2, object3'\")\n",
        "        print(\"  Example: 'cat, dog, person'\")\n",
        "\n",
        "    user_prompt = input(\"\\nPrompt: \")\n",
        "\n",
        "    print(f\"\\nSegmenting '{user_prompt}'...\")\n",
        "    user_masks, user_boxes, user_labels = pipeline.segment_from_text(\n",
        "        user_image, user_prompt, box_threshold=0.15\n",
        "    )\n",
        "\n",
        "    visualize_results(user_image, user_masks, user_boxes, user_labels, user_prompt)\n"
      ],
      "metadata": {
        "id": "G-ubPGY-rYrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 12: BONUS - Video Segmentation\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BONUS: VIDEO OBJECT SEGMENTATION\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"Downloading sample video...\")\n",
        "!wget -q -O sample_video.mp4 \"https://github.com/facebookresearch/segment-anything-2/raw/main/notebooks/videos/bedroom.mp4\"\n",
        "print(\"✓ Video downloaded\\n\")\n",
        "\n",
        "def extract_frames(video_path: str, output_dir: str = \"video_frames\") -> List[str]:\n",
        "    \"\"\"Extract frames from video\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_paths = []\n",
        "    frame_idx = 0\n",
        "\n",
        "    pbar = tqdm(desc=\"Extracting frames\")\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_path = f\"{output_dir}/frame_{frame_idx:05d}.jpg\"\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "        frame_paths.append(frame_path)\n",
        "        frame_idx += 1\n",
        "        pbar.update(1)\n",
        "\n",
        "    cap.release()\n",
        "    pbar.close()\n",
        "    return frame_paths\n",
        "\n",
        "print(\"Extracting frames...\")\n",
        "frame_paths = extract_frames(\"sample_video.mp4\")\n",
        "print(f\"✓ Extracted {len(frame_paths)} frames\\n\")\n",
        "\n",
        "# Detect in first frame\n",
        "first_frame = cv2.imread(frame_paths[0])\n",
        "first_frame_rgb = cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "video_prompt = \"bed\"  # Adjust based on video\n",
        "print(f\"Detecting '{video_prompt}' in first frame...\")\n",
        "\n",
        "boxes, _, labels = pipeline.detect_objects(first_frame_rgb, video_prompt, box_threshold=0.15)\n",
        "\n",
        "if len(boxes) > 0:\n",
        "    print(f\"✓ Found {len(boxes)} object(s): {labels}\\n\")\n",
        "\n",
        "    print(\"Propagating mask across video...\")\n",
        "\n",
        "    inference_state = sam2_video_predictor.init_state(video_path=\"video_frames\")\n",
        "    sam2_video_predictor.reset_state(inference_state)\n",
        "\n",
        "    _, out_obj_ids, out_mask_logits = sam2_video_predictor.add_new_points_or_box(\n",
        "        inference_state=inference_state,\n",
        "        frame_idx=0,\n",
        "        obj_id=1,\n",
        "        box=boxes[0],\n",
        "    )\n",
        "\n",
        "    video_segments = {}\n",
        "    for out_frame_idx, out_obj_ids, out_mask_logits in sam2_video_predictor.propagate_in_video(inference_state):\n",
        "        video_segments[out_frame_idx] = {\n",
        "            out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
        "            for i, out_obj_id in enumerate(out_obj_ids)\n",
        "        }\n",
        "\n",
        "    print(f\"✓ Segmented {len(video_segments)} frames\\n\")\n",
        "\n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    sample_indices = np.linspace(0, len(frame_paths)-1, 8, dtype=int)\n",
        "\n",
        "    for idx, frame_idx in enumerate(sample_indices):\n",
        "        frame = cv2.imread(frame_paths[frame_idx])\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if frame_idx in video_segments:\n",
        "            mask = video_segments[frame_idx][1][0]\n",
        "            overlay = frame.copy()\n",
        "            overlay[mask] = overlay[mask] * 0.5 + np.array([0, 255, 0]) * 0.5\n",
        "\n",
        "            contours, _ = cv2.findContours(mask.astype(np.uint8),\n",
        "                                          cv2.RETR_EXTERNAL,\n",
        "                                          cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(overlay, contours, -1, (255, 255, 0), 2)\n",
        "            axes[idx].imshow(overlay)\n",
        "        else:\n",
        "            axes[idx].imshow(frame)\n",
        "\n",
        "        axes[idx].set_title(f\"Frame {frame_idx}\", fontsize=12, fontweight='bold')\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.suptitle(f\"Video Segmentation: '{video_prompt}'\",\n",
        "                fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n✓ Video segmentation complete!\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠ No objects detected in first frame. Try different prompt.\")\n"
      ],
      "metadata": {
        "id": "TOpNjg96reZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 13: Summary\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "if not USE_FALLBACK:\n",
        "    print(\"✓ Using GroundingDINO + SAM 2\")\n",
        "else:\n",
        "    print(\"✓ Using OWL-ViT (fallback) + SAM 2\")\n",
        "print(\"✓ Text-driven image segmentation working\")\n",
        "print(\"✓ Video segmentation with temporal propagation\")\n",
        "print(\"\\nTIPS:\")\n",
        "print(\"- Be specific in prompts: 'red car' > 'car'\")\n",
        "print(\"- Lower box_threshold (0.1-0.2) if detecting too few objects\")\n",
        "print(\"- Raise box_threshold (0.3-0.4) if too many false positives\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "iGOD5yp7rly6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}