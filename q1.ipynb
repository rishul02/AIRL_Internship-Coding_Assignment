{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNun2ljCVww9wHdazC5bZ/l"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wFTcsMdSRoZU"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!pip install -q einops torchvision\n",
        "!pip install -q timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange\n",
        "\n",
        "class Config:\n",
        "    img_size = 32\n",
        "    patch_size = 4\n",
        "    in_channels = 3\n",
        "    num_classes = 10\n",
        "    embed_dim = 384\n",
        "    depth = 12\n",
        "    num_heads = 8\n",
        "    mlp_ratio = 4\n",
        "    dropout = 0.1\n",
        "    attn_dropout = 0.1\n",
        "    batch_size = 128\n",
        "    epochs = 200\n",
        "    lr = 3e-4\n",
        "    weight_decay = 0.05\n",
        "    warmup_epochs = 5\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "cfg = Config()\n",
        "print(\"Using device:\", cfg.device)\n"
      ],
      "metadata": {
        "id": "HsrvP3p5SNOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = (0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)\n",
        "from timm.data.mixup import Mixup\n",
        "mixup_fn = Mixup(mixup_alpha=0.8, cutmix_alpha=1.0, label_smoothing=0.1, num_classes=cfg.num_classes)\n",
        "\n",
        "train_tf = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "    transforms.RandomErasing(p=0.25),\n",
        "])\n",
        "\n",
        "test_tf = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "train_ds = datasets.CIFAR10(\"./data\", train=True, download=True, transform=train_tf)\n",
        "test_ds  = datasets.CIFAR10(\"./data\", train=False, download=True, transform=test_tf)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "-6ol8s3USbLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_chans, embed_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)          # (B, E, H/ps, W/ps)\n",
        "        x = x.flatten(2).transpose(1,2)  # (B, n_patches, E)\n",
        "        return x\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, attn_dropout=0., proj_dropout=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=True)\n",
        "        self.attn_drop = nn.Dropout(attn_dropout)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_dropout)\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B,N,3,self.num_heads,self.head_dim).permute(2,0,3,1,4)\n",
        "        q,k,v = qkv[0], qkv[1], qkv[2]\n",
        "        attn = (q @ k.transpose(-2,-1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1,2).reshape(B,N,C)\n",
        "        return self.proj_drop(self.proj(x))\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim, mlp_ratio=4., dropout=0.):\n",
        "        super().__init__()\n",
        "        hidden = int(dim * mlp_ratio)\n",
        "        self.fc1 = nn.Linear(dim, hidden)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden, dim)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self,x):\n",
        "        return self.drop(self.fc2(self.drop(self.act(self.fc1(x)))))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., dropout=0., attn_dropout=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadSelfAttention(dim,num_heads,attn_dropout,dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = MLP(dim,mlp_ratio,dropout)\n",
        "    def forward(self,x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_chans, num_classes,\n",
        "                 embed_dim, depth, num_heads, mlp_ratio, dropout, attn_dropout):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        n_patches = self.patch_embed.num_patches\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1,n_patches+1,embed_dim))\n",
        "        self.pos_drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim,num_heads,mlp_ratio,dropout,attn_dropout) for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim,num_classes)\n",
        "        nn.init.trunc_normal_(self.pos_embed,std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token,std=0.02)\n",
        "    def forward(self,x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B,-1,-1)\n",
        "        x = torch.cat((cls_tokens,x),dim=1)\n",
        "        x = self.pos_drop(x + self.pos_embed)\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:,0])\n"
      ],
      "metadata": {
        "id": "TGabYZm-S9VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer(model, lr, wd):\n",
        "    param_groups = [\n",
        "        {\"params\":[p for n,p in model.named_parameters() if p.requires_grad and 'bias' not in n and 'norm' not in n],\"weight_decay\":wd},\n",
        "        {\"params\":[p for n,p in model.named_parameters() if p.requires_grad and ('bias' in n or 'norm' in n)],\"weight_decay\":0.0},\n",
        "    ]\n",
        "    return optim.AdamW(param_groups, lr=lr, betas=(0.9,0.999))\n",
        "\n",
        "def get_cosine_with_warmup(optimizer, warmup_steps, total_steps):\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps: return float(step)/float(max(1,warmup_steps))\n",
        "        progress = float(step-warmup_steps)/float(max(1,total_steps-warmup_steps))\n",
        "        return 0.5*(1.0+math.cos(math.pi*progress))\n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n"
      ],
      "metadata": {
        "id": "6u-tDOaVS9Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model,loader,device):\n",
        "    model.eval(); total=0; correct=0; loss_sum=0\n",
        "    for imgs,labels in loader:\n",
        "        imgs,labels=imgs.to(device),labels.to(device)\n",
        "        out=model(imgs); loss=criterion(out,labels)\n",
        "        loss_sum+=loss.item()*imgs.size(0)\n",
        "        pred=out.argmax(1); correct+=(pred==labels).sum().item()\n",
        "        total+=imgs.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "def train_one_epoch(model,loader,optimizer,scaler,device,scheduler=None):\n",
        "    model.train(); running_loss=0\n",
        "    pbar=tqdm(loader,desc=\"Train\")\n",
        "    for imgs,labels in pbar:\n",
        "        imgs,labels=imgs.to(device),labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mixup_fn is not None:\n",
        "           imgs, labels = mixup_fn(imgs, labels)\n",
        "\n",
        "        with autocast():\n",
        "            out = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "        scaler.step(optimizer); scaler.update()\n",
        "        if scheduler: scheduler.step()\n",
        "        running_loss+=loss.item()*imgs.size(0)\n",
        "        pbar.set_postfix({\"loss\":running_loss/((pbar.n+1)*loader.batch_size)})\n",
        "    return running_loss/len(loader.dataset)\n"
      ],
      "metadata": {
        "id": "5BU4IhErS9BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = cfg.device\n",
        "model = VisionTransformer(cfg.img_size,cfg.patch_size,cfg.in_channels,\n",
        "                          cfg.num_classes,cfg.embed_dim,cfg.depth,cfg.num_heads,\n",
        "                          cfg.mlp_ratio,cfg.dropout,cfg.attn_dropout).to(device)\n",
        "\n",
        "optimizer=get_optimizer(model,cfg.lr,cfg.weight_decay)\n",
        "total_steps=len(train_loader)*cfg.epochs\n",
        "warmup_steps=cfg.warmup_epochs*len(train_loader)\n",
        "scheduler=get_cosine_with_warmup(optimizer,warmup_steps,total_steps)\n",
        "scaler=GradScaler()\n",
        "\n",
        "best_acc=0; save_path=\"best_vit_cifar10.pth\"\n",
        "\n",
        "for epoch in range(cfg.epochs):\n",
        "    print(f\"Epoch {epoch+1}/{cfg.epochs}\")\n",
        "    train_loss=train_one_epoch(model,train_loader,optimizer,scaler,device,scheduler)\n",
        "    val_loss,val_acc=evaluate(model,test_loader,device)\n",
        "    print(f\"Train Loss:{train_loss:.4f} | Val Loss:{val_loss:.4f} | Val Acc:{val_acc*100:.2f}%\")\n",
        "    if val_acc>best_acc:\n",
        "        best_acc=val_acc\n",
        "        torch.save({\"model_state\":model.state_dict(),\"acc\":best_acc,\"epoch\":epoch},save_path)\n",
        "        print(f\"✓ Saved Best: {best_acc*100:.2f}%\")\n",
        "print(\"Training finished. Best Accuracy:\",best_acc*100)\n"
      ],
      "metadata": {
        "id": "NbHgDjw_TEfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qTr-RVj6TEWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PyrDsVsuSMLh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}